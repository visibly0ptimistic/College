{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDwNAZEY9Xgr"
      },
      "source": [
        "The MNIST database of handwritten digits.\n",
        "\n",
        "label\n",
        "call_split\n",
        "bar_chart\n",
        "70,000 items\n",
        "Value\n",
        "arrow_drop_up\n",
        "Count\n",
        "0\n",
        "6,903\n",
        "1\n",
        "7,877\n",
        "2\n",
        "6,990\n",
        "3\n",
        "7,141\n",
        "4\n",
        "6,824\n",
        "5\n",
        "6,313\n",
        "6\n",
        "6,876\n",
        "7\n",
        "7,293\n",
        "8\n",
        "6,825\n",
        "9\n",
        "6,958\n",
        "split\n",
        "call_split\n",
        "bar_chart\n",
        "70,000 items\n",
        "Value\n",
        "arrow_drop_up\n",
        "Count\n",
        "test\n",
        "10,000\n",
        "train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m9hmDV_k1elA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "547/547 [==============================] - 64s 111ms/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9381 - val_loss: 0.0608 - val_sparse_categorical_accuracy: 0.9808\n",
            "Epoch 2/15\n",
            "547/547 [==============================] - 62s 113ms/step - loss: 0.0608 - sparse_categorical_accuracy: 0.9810 - val_loss: 0.0499 - val_sparse_categorical_accuracy: 0.9849\n",
            "Epoch 3/15\n",
            "547/547 [==============================] - 62s 114ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9871 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.9881\n",
            "Epoch 4/15\n",
            "547/547 [==============================] - 64s 117ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0408 - val_sparse_categorical_accuracy: 0.9879\n",
            "Epoch 5/15\n",
            "547/547 [==============================] - 61s 111ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0418 - val_sparse_categorical_accuracy: 0.9879\n",
            "Epoch 6/15\n",
            "547/547 [==============================] - 66s 121ms/step - loss: 0.0213 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0375 - val_sparse_categorical_accuracy: 0.9883\n",
            "Epoch 7/15\n",
            "547/547 [==============================] - 65s 118ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0403 - val_sparse_categorical_accuracy: 0.9893\n",
            "Epoch 8/15\n",
            "547/547 [==============================] - 70s 127ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0425 - val_sparse_categorical_accuracy: 0.9890\n",
            "Epoch 9/15\n",
            "547/547 [==============================] - 65s 119ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.0390 - val_sparse_categorical_accuracy: 0.9902\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0539 - sparse_categorical_accuracy: 0.9886\n",
            "test loss, test acc: [0.05394825339317322, 0.9885988831520081]\n",
            "95% confidence interval for the accuracy: [0.9865, 0.9907]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import scipy.stats as stats\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# You cannot modify from here until it is indicated by a comment\n",
        "(test_data),test_data_info=tfds.load('mnist',split='test',with_info=True,as_supervised=True)\n",
        "\n",
        "(train_data),ds_info=tfds.load('mnist',split=['train[10000:45000]'],with_info=True,as_supervised=True)\n",
        "\n",
        "def getnewtst():\n",
        "  (new_test),new_test_info=tfds.load('mnist',split=['train[0:9999]'],with_info=True,as_supervised=True)\n",
        "  new_test = new_test[0].map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  new_test = new_test.batch(64)\n",
        "  new_test = new_test.cache()\n",
        "  new_test = new_test.prefetch(tf.data.AUTOTUNE)\n",
        "  return new_test\n",
        "\n",
        "\n",
        "# Can modify code now below this comment\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\n",
        "  The model wants the float and tfds gives you 0-255.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "\n",
        "train_data = train_data[0].map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_data = train_data.cache()\n",
        "train_data = train_data.shuffle(ds_info.splits['train'].num_examples)\n",
        "train_data = train_data.batch(64)\n",
        "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "test_data = test_data.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(128)\n",
        "test_data = test_data.cache()\n",
        "test_data = test_data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# in order to improve the performance, I increased the complexity of the convolutional neural network. By adding more convolutional and dense layers, the network becomes deeper and can learn more patterns and features. With a more complex architecture, the network has the potential to achieve better accuracy by capturing more detailed information from the input images.\n",
        "\n",
        "# I incorporated Dropout into the model. Which randomly sets a fraction rate of input units to 0 at each update during training time, this helps prevent overfitting. The fraction rate is defined as the parameter in Dropout().\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), \n",
        "                           activation='relu'\n",
        "                           ),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# the previous optimizer used was a stochastic gradient descent with a momentum of 0.3. I am trying Adam as an optimizer, since it often provides better convergence and performance. Adam optimizer uses adaptive learning rates and momentum, which can lead to faster convergence and potentially better accuracy.\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "\n",
        "# the code previously trained the model for 7 epochs. I increased the number of epochs to allow the model more training iterations. Increasing the number of epochs allows the model to see the data multiple times, which can lead to improved accuracy as the model learns from the data more extensively.\n",
        "\n",
        "# increasing the number of epochs gives the model with more opportunities to learn from the data, however it also increases the risk of overfitting. Overfitting happens when the model starts to memorize the training data, which leads to high accuracy on the training set but poor performance on unseen data. To mitigate this, I introduced an early stopping callback. Early stopping monitors, in this case 'val_loss', across epochs during training. If the model's performance on the validation set doesn't improve for a specified number of epochs, or 'patience', the training is halted. This prevents overfitting and reduces computational cost by avoiding unnecessary epochs of training.\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.fit(\n",
        "    train_data,\n",
        "    epochs=15,\n",
        "    validation_data=test_data,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "results = model.evaluate(getnewtst())\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "\n",
        "# I introduced a function to calculate the confidence interval, which helps determine the bounds of accuracy the model can achieve. using the Z-score for 95% confidence interval\n",
        "def compute_confidence_interval(accuracy, n):\n",
        "    z = stats.norm.ppf(1 - (1 - 0.95) / 2)\n",
        "    interval = z * np.sqrt((accuracy * (1 - accuracy)) / n)\n",
        "    lower_bound = accuracy - interval\n",
        "    upper_bound = accuracy + interval\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "\n",
        "# confidence interval\n",
        "accuracy = results[1]  # Test accuracy\n",
        "n = 10000  # Number of test examples\n",
        "lower_bound, upper_bound = compute_confidence_interval(accuracy, n)\n",
        "print(f\"95% confidence interval for the accuracy: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
